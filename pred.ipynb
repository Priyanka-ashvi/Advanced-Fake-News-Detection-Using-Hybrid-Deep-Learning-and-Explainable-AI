{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['bert.embeddings.position_ids']\n",
      "Unexpected keys: []\n",
      "Prediction: Real News\n",
      "Probabilities: [0.00302568 0.99697435]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, RobertaTokenizer, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, RobertaTokenizer, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "def load_model(model_path, model_class, tokenizer_class):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Initialize the model and tokenizer\n",
    "    model = model_class.from_pretrained(\"bert-base-uncased\" if \"bert\" in model_path else \"roberta-base\", \n",
    "                                        num_labels=2, \n",
    "                                        output_attentions=False,\n",
    "                                        output_hidden_states=False,\n",
    "                                        ignore_mismatched_sizes=True)  # Add this to ignore size mismatches\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained(\"bert-base-uncased\" if \"bert\" in model_path else \"roberta-base\")\n",
    "    \n",
    "    # Load the trained model parameters, ignoring missing keys\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    print(\"Missing keys:\", missing_keys)\n",
    "    print(\"Unexpected keys:\", unexpected_keys)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict(input_text, model, tokenizer):\n",
    "    # Encode the input text\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text=input_text,\n",
    "                        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                        max_length=64,  # Pad & truncate all sentences.\n",
    "                        pad_to_max_length=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',  # Return PyTorch tensors\n",
    "                   )\n",
    "    \n",
    "    # Extract inputs from the encoded dictionary\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    \n",
    "    # Model prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    predicted_prob = torch.softmax(logits, dim=1).numpy().flatten()  # Convert logits to probabilities\n",
    "    prediction = torch.argmax(logits, dim=1).numpy().flatten()[0]  # Get the predicted class index\n",
    "    \n",
    "    return prediction, predicted_prob\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = 'bert.bin'  # or 'roberta_model.bin'\n",
    "model_class = BertForSequenceClassification  # or RobertaForSequenceClassification for RoBERTa\n",
    "tokenizer_class = BertTokenizer  # or RobertaTokenizer for RoBERTa\n",
    "\n",
    "model, tokenizer = load_model(model_path, model_class, tokenizer_class)\n",
    "\n",
    "# Predict user input\n",
    "user_input = \"JAKARTA (Reuters) - Indonesia will buy 11 Sukhoi fighter jets worth $1.14 billion from Russia in exchange for cash and Indonesian commodities, two cabinet ministers said on Tuesday. The Southeast Asian country has pledged to ship up to $570 million worth of commodities in addition to cash to pay for the Suhkoi SU-35 fighter jets, which are expected to be delivered in stages starting in two years. Indonesian Trade Minister Enggartiasto Lukita said in a joint statement with Defence Minister Ryamizard Ryacudu that details of the type and volume of commodities were  still being negotiated . Previously he had said the exports could include palm oil, tea, and coffee. The deal is expected to be finalised soon between Indonesian state trading company PT Perusahaan Perdangangan Indonesia and Russian state conglomerate Rostec. Russia is currently facing a new round of U.S.-imposed trade sanctions. Meanwhile, Southeast Asia s largest economy is trying to promote its palm oil products amid threats of a cut in consumption by European Union countries. Indonesia is also trying to modernize its ageing air force after a string of military aviation accidents. Indonesia, which had a $411 million trade surplus with Russia in 2016, wants to expand bilateral cooperation in tourism, education, energy, technology and aviation among others. \"\n",
    "\n",
    "prediction, probabilities = predict(user_input, model, tokenizer)\n",
    "\n",
    "print(\"Prediction:\", \"Real News\" if prediction == 1 else \"Fake News\")\n",
    "print(\"Probabilities:\", probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['bert.embeddings.position_ids']\n",
      "Unexpected keys: []\n",
      "Prediction: Fake News\n",
      "Probabilities: [0.6148449  0.38515505]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0871\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, RobertaTokenizer, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, RobertaTokenizer, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "def load_model(model_path, model_class, tokenizer_class):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Initialize the model and tokenizer\n",
    "    model = model_class.from_pretrained(\"bert-base-uncased\" if \"bert\" in model_path else \"roberta-base\", \n",
    "                                        num_labels=2, \n",
    "                                        output_attentions=False,\n",
    "                                        output_hidden_states=False,\n",
    "                                        ignore_mismatched_sizes=True)  # Add this to ignore size mismatches\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained(\"bert-base-uncased\" if \"bert\" in model_path else \"roberta-base\")\n",
    "    \n",
    "    # Load the trained model parameters, ignoring missing keys\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    print(\"Missing keys:\", missing_keys)\n",
    "    print(\"Unexpected keys:\", unexpected_keys)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict(input_text, model, tokenizer):\n",
    "    # Encode the input text\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text=input_text,\n",
    "                        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                        max_length=64,  # Pad & truncate all sentences.\n",
    "                        pad_to_max_length=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',  # Return PyTorch tensors\n",
    "                   )\n",
    "    \n",
    "    # Extract inputs from the encoded dictionary\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    \n",
    "    # Model prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    predicted_prob = torch.softmax(logits, dim=1).numpy().flatten()  # Convert logits to probabilities\n",
    "    prediction = torch.argmax(logits, dim=1).numpy().flatten()[0]  # Get the predicted class index\n",
    "    \n",
    "    return prediction, predicted_prob\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = 'roberta_model.bin'  # or 'roberta_model.bin'\n",
    "model_class = BertForSequenceClassification  # or RobertaForSequenceClassification for RoBERTa\n",
    "tokenizer_class = BertTokenizer  # or RobertaTokenizer for RoBERTa\n",
    "\n",
    "model, tokenizer = load_model(model_path, model_class, tokenizer_class)\n",
    "\n",
    "# Predict user input\n",
    "user_input = \"WEST PALM BEACH, Fla./WASHINGTON (Reuters) - The White House said on Friday it was set to kick off talks next week with Republican and Democratic congressional leaders on immigration policy, government spending and other issues that need to be wrapped up early in the new year. The expected flurry of legislative activity comes as Republicans and Democrats begin to set the stage for midterm congressional elections in November. President Donald Trumpâ€™s Republican Party is eager to maintain control of Congress while Democrats look for openings to wrest seats away in the Senate and the House of Representatives. On Wednesday, Trumpâ€™s budget chief Mick Mulvaney and legislative affairs director Marc Short will meet with Senate Majority Leader Mitch McConnell and House Speaker Paul Ryan - both Republicans - and their Democratic counterparts, Senator Chuck Schumer and Representative Nancy Pelosi, the White House said. That will be followed up with a weekend of strategy sessions for Trump, McConnell and Ryan on Jan. 6 and 7 at the Camp David presidential retreat in Maryland, according to the White House. The Senate returns to work on Jan. 3 and the House on Jan. 8. Congress passed a short-term government funding bill last week before taking its Christmas break, but needs to come to an agreement on defense spending and various domestic programs by Jan. 19, or the government will shut down. Also on the agenda for lawmakers is disaster aid for people hit by hurricanes in Puerto Rico, Texas and Florida, and by wildfires in California. The House passed an $81 billion package in December, which the Senate did not take up. The White House has asked for a smaller figure, $44 billion. Deadlines also loom for soon-to-expire protections for young adult immigrants who entered the country illegally as children, known as â€œDreamers.â€ In September, Trump ended Democratic former President Barack Obamaâ€™s Deferred Action for Childhood Arrivals (DACA) program, which protected Dreamers from deportation and provided work permits, effective in March, giving Congress until then to devise a long-term solution. Democrats, some Republicans and a number of large companies have pushed for DACA protections to continue. Trump and other Republicans have said that will not happen without Congress approving broader immigration policy changes and tougher border security. Democrats oppose funding for a wall promised by Trump along the U.S.-Mexican border.  â€œThe Democrats have been told, and fully understand, that there can be no DACA without the desperately needed WALL at the Southern Border and an END to the horrible Chain Migration & ridiculous Lottery System of Immigration etc,â€ Trump said in a Twitter post on Friday. Trump wants to overhaul immigration rules for extended families and others seeking to live in the United States. Republican U.S. Senator Jeff Flake, a frequent critic of the president, said he would work with Trump to protect Dreamers. â€œWe can fix DACA in a way that beefs up border security, stops chain migration for the DREAMers, and addresses the unfairness of the diversity lottery. If POTUS (Trump) wants to protect these kids, we want to help him keep that promise,â€ Flake wrote on Twitter. Congress in early 2018 also must raise the U.S. debt ceiling to avoid a government default. The U.S. Treasury would exhaust all of its borrowing options and run dry of cash to pay its bills by late March or early April if Congress does not raise the debt ceiling before then, according to the nonpartisan Congressional Budget Office. Trump, who won his first major legislative victory with the passage of a major tax overhaul this month, has also promised a major infrastructure plan. \"\n",
    "\n",
    "prediction, probabilities = predict(user_input, model, tokenizer)\n",
    "\n",
    "print(\"Prediction:\", \"Real News\" if prediction == 1 else \"Fake News\")\n",
    "print(\"Probabilities:\", probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
